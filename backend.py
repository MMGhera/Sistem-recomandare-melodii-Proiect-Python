#V4V4
import sys
import os
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
import json
import torch
import numpy as np
import librosa
# import ai.dataset
# # Ãi spunem lui Python: "CÃ¢nd cineva cautÄƒ 'dataset', dÄƒ-i 'ai.dataset'"
# sys.modules['dataset'] = ai.dataset

# --- SETUP IMPORTURI AI ---
sys.path.append(os.path.dirname(__file__))

# 1. ImportÄƒm modulul dataset (cel care este folderul colegului)
import dataset

try:
    # ImportÄƒm componentele AI din locaÈ›ia lor realÄƒ
    from ai.model import MusiCNN, INSTRUMENT_MAP
    # ImportÄƒm È˜I MelConfig explicit
    from ai.dataset import generate_melspectrogram, MelConfig

    # Ãi spunem Python-ului: "DacÄƒ cineva cautÄƒ MelConfig Ã®n dataset, dÄƒ-i-l pe Äƒsta!"
    dataset.MelConfig = MelConfig

except ImportError as e:
    print(f"âš ï¸ Warning importuri AI: {e}")

from dataset.extract import get_features_from_path

import requests
import uuid # Pentru nume unice la fiÈ™iere temporare
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from contextlib import asynccontextmanager
from typing import List
from sqlalchemy.orm import Session
from sklearn.preprocessing import normalize

# --- IMPORTURI DB ---
from models import create_tables, get_db, User, Song, UserPreference, SessionLocal

# --- SETUP IMPORTURI AI ---
sys.path.append(os.path.dirname(__file__))
try:
    from ai.model import MusiCNN, INSTRUMENT_MAP
    from ai.dataset import generate_melspectrogram
except ImportError:
    pass  # IgnorÄƒm dacÄƒ nu merge importul local, doar pentru test



# --- CONFIGURARE AI ---
AI_MODEL_PATH = os.path.join("ai", "checkpoints", "big_sample_rate", "best.pt")
AUDIO_LIBRARY_PATH = "audio_library"
ai_context = {"model": None, "config": None, "device": None}


# --- LIFESPAN ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # 1. IniÈ›ializÄƒm Baza de Date SQL
    create_tables()
    print("ğŸ’¾ Baza de date SQL conectatÄƒ.")

    # 2. ÃncÄƒrcÄƒm AI-ul
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if os.path.exists(AI_MODEL_PATH):
        try:
            state = torch.load(AI_MODEL_PATH, map_location=device, weights_only=False)
            mel_conf = state["mel_config"]
            instrument_list = state["instrument_list"]
            model = MusiCNN(num_classes=len(instrument_list), num_mels=mel_conf.n_mels)
            model.load_state_dict(state["model"])
            model.to(device)
            model.eval()

            ai_context.update({"model": model, "device": device, "config": {
                "mel_config": mel_conf, "frames_per_window": state["frames_per_window"]
            }})
            print("âœ… Model AI Ã®ncÄƒrcat.")
        except Exception as e:
            print(f"âŒ Eroare AI: {e}")

    yield


app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)


# --- LOGICA AI (AceeaÈ™i ca Ã®nainte) ---

def analyze_audio_file(file_path):
    # VerificÄƒri preliminare AI
    model = ai_context["model"]
    cfg = ai_context["config"]
    device = ai_context["device"]

    if model is None:
        return None

    try:
        # --- PARTEA 1: AI (Deep Learning - MusiCNN) ---
        # 1. ÃncÄƒrcare Audio
        audio, sr = librosa.load(file_path, sr=None)  # ÃncÄƒrcÄƒm tot fiÈ™ierul pentru AI

        # 2. Resample
        audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=cfg["mel_config"].sample_rate)

        # 3. SpectrogramÄƒ
        mel = generate_melspectrogram(audio_resampled, cfg["mel_config"])

        # 4. Chunking
        frames_win = cfg["frames_per_window"]
        chunks = []
        total_frames = mel.shape[1]

        for offset in range(0, total_frames, frames_win):
            chunk = mel[:, offset:offset + frames_win]
            if chunk.shape[1] < frames_win:
                pad_width = frames_win - chunk.shape[1]
                chunk = np.pad(chunk, ((0, 0), (0, pad_width)), mode='constant')
            chunks.append(chunk)

        if not chunks:
            ai_vector = np.zeros(128)  # Fallback dacÄƒ e gol (depinde de output-ul modelului tÄƒu)
        else:
            # 5. PredicÈ›ie
            batch_tensor = np.stack(chunks)
            batch_tensor = torch.from_numpy(batch_tensor).float().unsqueeze(1).to(device)

            with torch.no_grad():
                logits = model(batch_tensor)
                probs = logits.cpu().numpy()

            # 6. Max Pooling
            ai_vector = np.max(probs, axis=0)

        # --- PARTEA 2: CLASIC (Metoda Colegului) ---
        # Aici apelÄƒm funcÈ›ia importatÄƒ din dataset/extract.py
        classic_vector = get_features_from_path(file_path)

        # --- PARTEA 3: FUZIUNEA (Concatenare + Normalizare) ---

        # Reshape pentru sklearn (vrea matrice 2D)
        ai_vector_2d = ai_vector.reshape(1, -1)
        classic_vector_2d = classic_vector.reshape(1, -1)

        # Normalizare L2 (aduce vectorii la scarÄƒ comunÄƒ)
        ai_norm = normalize(ai_vector_2d, axis=1, norm='l2')[0]
        classic_norm = normalize(classic_vector_2d, axis=1, norm='l2')[0]

        # Concatenare
        final_vector = np.concatenate([ai_norm, classic_norm])

        print(f"âœ… Analizat hibrid: {os.path.basename(file_path)} (Len: {len(final_vector)})")
        return final_vector.tolist()

    except Exception as e:
        print(f"âš ï¸ Eroare la analizÄƒ hibridÄƒ {file_path}: {e}")
        return None


# --- MODELE PYDANTIC (Pentru API) ---
class LoginRequest(BaseModel):
    username: str


class PrefsRequest(BaseModel):
    username: str
    songs: list[str]


# --- ENDPOINTS NOI CU SQL ---

@app.post("/login")
def login(request: LoginRequest, db: Session = Depends(get_db)):
    # CÄƒutÄƒm userul Ã®n SQL
    user = db.query(User).filter(User.username == request.username).first()
    if not user:
        # CreÄƒm user nou
        new_user = User(username=request.username)
        db.add(new_user)
        db.commit()
    return {"status": "ok", "username": request.username}


@app.get("/prefs/{username}")
def get_prefs(username: str, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.username == username).first()
    if not user:
        return {"songs": []}

    # Extragem numele melodiilor din tabelul de preferinÈ›e
    song_titles = [pref.song.title for pref in user.preferences]
    return {"songs": song_titles}


@app.post("/prefs")
def save_prefs(request: PrefsRequest, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.username == request.username).first()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    # È˜tergem preferinÈ›ele vechi (cea mai simplÄƒ metodÄƒ de update)
    db.query(UserPreference).filter(UserPreference.user_id == user.id).delete()

    # AdÄƒugÄƒm noile preferinÈ›e
    for song_name in request.songs:
        # VerificÄƒm dacÄƒ melodia existÄƒ Ã®n baza de date
        song = db.query(Song).filter(Song.title == song_name).first()
        if song:
            new_pref = UserPreference(user_id=user.id, song_id=song.id)
            db.add(new_pref)

    db.commit()
    return {"status": "ok"}


@app.get("/autocomplete")
def autocomplete(q: str, db: Session = Depends(get_db)):
    # CÄƒutare SQL (LIKE %q%)
    songs = db.query(Song).filter(Song.title.contains(q)).limit(10).all()
    return [s.title for s in songs]


@app.post("/scan_library")
def scan_library(db: Session = Depends(get_db)):
    # ImportÄƒm logica de analizÄƒ din contextul global sau funcÈ›ia definitÄƒ
    # Nota: Trebuie sÄƒ incluzi funcÈ›ia analyze_audio_file completÄƒ Ã®n acest fiÈ™ier
    from ai.dataset import generate_melspectrogram  # Re-import pt siguranÈ›Äƒ

    if not os.path.exists(AUDIO_LIBRARY_PATH):
        return {"error": "No audio folder"}

    processed = 0
    files = os.listdir(AUDIO_LIBRARY_PATH)

    for file in files:
        if file.endswith((".mp3", ".wav", ".m4a")):
            song_name = os.path.splitext(file)[0]

            # VerificÄƒm dacÄƒ existÄƒ deja Ã®n SQL
            exists = db.query(Song).filter(Song.title == song_name).first()
            if not exists:
                print(f"ğŸµ Analizez: {song_name}...")
                full_path = os.path.join(AUDIO_LIBRARY_PATH, file)

                # AICI apelÄƒm AI-ul tÄƒu
                # vector = analyze_audio_file(full_path)
                # (Simulare vector pentru exemplul DB - tu decomenteazÄƒ analiza realÄƒ)
                #vector = [0.1, 0.2, 0.3]  # Placeholder dacÄƒ nu merge analiza pe moment
                try:
                    vector = analyze_audio_file(full_path)
                except Exception as e:
                    print(f"Eroare la analiza {song_name}: {e}")
                    vector = None

                if vector:
                    new_song = Song(
                        title=song_name,
                        vector_data=json.dumps(vector)  # SalvÄƒm vectorul ca text JSON
                    )
                    db.add(new_song)
                    processed += 1
                    db.commit()  # SalvÄƒm fiecare melodie pe rÃ¢nd

    return {"status": "completed", "new_songs": processed}


@app.get("/recommend/{username}")
def recommend(username: str, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.username == username).first()
    if not user or not user.preferences:
        return {"recommendations": ["AdaugÄƒ preferinÈ›e!"]}

    # 1. Construim profilul userului
    user_vectors = []
    for pref in user.preferences:
        user_vectors.append(json.loads(pref.song.vector_data))

    if not user_vectors:
        return {"recommendations": []}

    # Media vectorilor
    user_profile = np.mean(np.array(user_vectors), axis=0)

    # 2. LuÄƒm toate melodiile din DB
    all_songs = db.query(Song).all()
    scores = []

    my_song_ids = [p.song_id for p in user.preferences]

    for song in all_songs:
        if song.id in my_song_ids: continue  # Excludem ce ascultÄƒ deja

        song_vec = np.array(json.loads(song.vector_data))

        # Cosine Similarity
        similarity = np.dot(user_profile, song_vec) / (np.linalg.norm(user_profile) * np.linalg.norm(song_vec))

        if similarity > 0.35:  # Prag minim
            scores.append((song.title, similarity))

    scores.sort(key=lambda x: x[1], reverse=True)
    return {"recommendations": [s[0] for s in scores[:5]]}


# --- ENDPOINT NOU: ANALIZÄ‚ EXTERNÄ‚ LIVE ---
@app.get("/analyze_external")
def analyze_external(q: str, username: str, db: Session = Depends(get_db)):
    """
    CautÄƒ pe iTunes, analizeazÄƒ, SALVEAZÄ‚ Ã®n DB È™i adaugÄƒ la PREFERINÈšELE utilizatorului.
    """
    print(f"ğŸŒ {username} cautÄƒ È™i adaugÄƒ: {q}")

    # 1. CÄƒutare pe iTunes
    itunes_url = f"https://itunes.apple.com/search?term={q}&media=music&entity=song&limit=1"
    try:
        resp = requests.get(itunes_url).json()
        if not resp["results"]:
            return {"error": "Melodia nu a fost gÄƒsitÄƒ pe iTunes."}

        track = resp["results"][0]
        full_name = f"{track['artistName']} - {track['trackName']}"
        preview_url = track["previewUrl"]

        # 2. Gestionare Melodie Ã®n DB (VerificÄƒm / AdÄƒugÄƒm)
        song_in_db = db.query(Song).filter(Song.title == full_name).first()

        target_vector = None

        if song_in_db:
            print("âš¡ Melodia existÄƒ deja. O refolosim.")
            target_vector = json.loads(song_in_db.vector_data)
        else:
            # Nu existÄƒ -> O descÄƒrcÄƒm È™i analizÄƒm
            print("â¬‡ï¸ Descarc È™i analizez melodia nouÄƒ...")
            temp_path = os.path.join(AUDIO_LIBRARY_PATH, f"temp_{uuid.uuid4()}.m4a")
            os.makedirs(AUDIO_LIBRARY_PATH, exist_ok=True)

            r = requests.get(preview_url)
            with open(temp_path, 'wb') as f:
                f.write(r.content)

            target_vector = analyze_audio_file(temp_path)

            if os.path.exists(temp_path):
                os.remove(temp_path)  # È˜tergem fiÈ™ierul audio, pÄƒstrÄƒm doar matematica

            if target_vector:
                # SALVARE PERMANENTÄ‚ ÃN BAZA DE DATE
                song_in_db = Song(title=full_name, vector_data=json.dumps(target_vector))
                db.add(song_in_db)
                db.commit()
                db.refresh(song_in_db)
            else:
                return {"error": "AI-ul nu a putut analiza fiÈ™ierul."}

        # 3. AdÄƒugare la PreferinÈ›ele Utilizatorului (Link User <-> Song)
        user = db.query(User).filter(User.username == username).first()
        if user:
            # VerificÄƒm dacÄƒ nu o are deja
            existing_pref = db.query(UserPreference).filter(
                UserPreference.user_id == user.id,
                UserPreference.song_id == song_in_db.id
            ).first()

            if not existing_pref:
                new_pref = UserPreference(user_id=user.id, song_id=song_in_db.id)
                db.add(new_pref)
                db.commit()
                print(f"âœ… AdÄƒugat '{full_name}' la preferinÈ›ele lui {username}.")

    except Exception as e:
        return {"error": f"Eroare server: {str(e)}"}

    # 4. RecomandÄƒri (Bazat pe melodia tocmai adÄƒugatÄƒ)
    all_songs = db.query(Song).all()
    scores = []
    target_np = np.array(target_vector)

    for song in all_songs:
        if song.id == song_in_db.id: continue  # Nu ne recomandÄƒm pe noi Ã®nÈ™ine

        song_vec = np.array(json.loads(song.vector_data))
        similarity = np.dot(target_np, song_vec) / (np.linalg.norm(target_np) * np.linalg.norm(song_vec))
        scores.append((song.title, similarity))

    scores.sort(key=lambda x: x[1], reverse=True)

    return {
        "source_song": full_name,
        "recommendations": [s[0] for s in scores[:5]],
        "added_to_library": True
    }


@app.get("/itunes_autocomplete")
def itunes_autocomplete(q: str):
    """
    ReturneazÄƒ o listÄƒ scurtÄƒ de sugestii de la iTunes (Titlu + Artist).
    """
    if not q or len(q) < 2:
        return []

    # Cerem doar 5 rezultate pentru vitezÄƒ
    url = f"https://itunes.apple.com/search?term={q}&media=music&entity=song&limit=5"
    try:
        resp = requests.get(url).json()
        results = []
        for track in resp.get("results", []):
            # FormatÄƒm frumos: "Artist - PiesÄƒ"
            display_name = f"{track['artistName']} - {track['trackName']}"
            results.append(display_name)
        # EliminÄƒm duplicatele (set) È™i returnÄƒm lista
        return list(set(results))
    except:
        return []


# --- ENDPOINT È˜TERGERE PREFERINÈšÄ‚ ---
@app.delete("/pref")
def delete_pref(username: str, song: str, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.username == username).first()
    if not user:
        return {"error": "User not found"}

    # GÄƒsim melodia
    song_obj = db.query(Song).filter(Song.title == song).first()
    if song_obj:
        # È˜tergem legÄƒtura dintre user È™i melodie
        db.query(UserPreference).filter(
            UserPreference.user_id == user.id,
            UserPreference.song_id == song_obj.id
        ).delete()
        db.commit()
        print(f"ğŸ—‘ï¸ {username} a È™ters: {song}")
        return {"status": "deleted"}

    return {"status": "song not found (ignored)"}

if __name__ == "__main__":
    import uvicorn
    # Asta È›ine programul deschis È™i ascultÄƒ cereri
    uvicorn.run(app, host="127.0.0.1", port=8000)